#!/usr/bin/env python3
"""
Google Photoså…±æœ‰ãƒªãƒ³ã‚¯ã‹ã‚‰ç”»åƒURLã‚’æŠ½å‡ºã™ã‚‹ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import re
import requests
from urllib.parse import urlparse, parse_qs

def extract_google_photos_urls(shared_link):
    """
    Google Photoså…±æœ‰ãƒªãƒ³ã‚¯ã‹ã‚‰ç”»åƒURLã‚’æŠ½å‡º
    """
    print(f"ğŸ”— Google Photoså…±æœ‰ãƒªãƒ³ã‚¯: {shared_link}")
    
    try:
        # 1. ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆå…ˆURLã‚’å–å¾—
        response = requests.get(shared_link, allow_redirects=True)
        final_url = response.url
        print(f"ğŸ“ ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆå…ˆ: {final_url}")
        
        # 2. ãƒšãƒ¼ã‚¸å†…å®¹ã‚’å–å¾—
        html_content = response.text
        
        # 3. ãƒ•ãƒ«ã‚µã‚¤ã‚ºç”»åƒURLãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢
        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: ãƒ•ãƒ«ã‚µã‚¤ã‚ºç”»åƒï¼ˆw4238-h2382ç­‰ã®å¤§ããªã‚µã‚¤ã‚ºï¼‰
        fullsize_pattern = r'https://lh[0-9]+\.googleusercontent\.com/pw/[A-Za-z0-9_-]+=w[3-9][0-9]{3}-h[0-9]+-s-no-gm\?authuser=[0-9]+'
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: authuserç„¡ã—ã®ãƒ•ãƒ«ã‚µã‚¤ã‚º
        fullsize_pattern2 = r'https://lh[0-9]+\.googleusercontent\.com/pw/[A-Za-z0-9_-]+=w[3-9][0-9]{3}-h[0-9]+-s-no-gm'
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³3: ãƒ™ãƒ¼ã‚¹URLï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç„¡ã—ï¼‰ã‹ã‚‰ãƒ•ãƒ«ã‚µã‚¤ã‚ºã‚’æ¨æ¸¬
        base_pattern = r'https://lh[0-9]+\.googleusercontent\.com/pw/[A-Za-z0-9_-]+'
        
        # ãƒ•ãƒ«ã‚µã‚¤ã‚ºURLã‚’æŠ½å‡º
        fullsize_urls1 = re.findall(fullsize_pattern, html_content)
        fullsize_urls2 = re.findall(fullsize_pattern2, html_content)
        base_urls = re.findall(base_pattern, html_content)
        
        # ãƒ•ãƒ«ã‚µã‚¤ã‚ºURLãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã¯ãã‚Œã‚’ä½¿ç”¨
        if fullsize_urls1 or fullsize_urls2:
            all_urls = list(set(fullsize_urls1 + fullsize_urls2))
            print(f"âœ… ãƒ•ãƒ«ã‚µã‚¤ã‚ºç”»åƒURLã‚’ç™ºè¦‹")
        else:
            # ãƒ•ãƒ«ã‚µã‚¤ã‚ºãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ãƒ™ãƒ¼ã‚¹URLã‚’ä½¿ç”¨
            all_urls = list(set(base_urls))
            print(f"âš ï¸  ãƒ•ãƒ«ã‚µã‚¤ã‚ºURLãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€ãƒ™ãƒ¼ã‚¹URLã‚’ä½¿ç”¨")
        
        # ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ç”»åƒï¼ˆ/a/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼‰ã‚’é™¤å¤–
        filtered_urls = [url for url in all_urls if '/a/' not in url]
        
        if len(filtered_urls) != len(all_urls):
            excluded_count = len(all_urls) - len(filtered_urls)
            print(f"ğŸš« ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ç”»åƒã‚’é™¤å¤–: {excluded_count}å€‹")
        
        all_urls = filtered_urls
        
        print(f"ğŸ–¼ï¸  æŠ½å‡ºã•ã‚ŒãŸç”»åƒURLæ•°: {len(all_urls)}")
        
        if all_urls:
            print("\nğŸ“‹ æŠ½å‡ºã•ã‚ŒãŸURLä¸€è¦§:")
            for i, url in enumerate(all_urls, 1):
                print(f"{i:2d}. {url}")
        else:
            print("âŒ ç”»åƒURLãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            
            # ãƒ‡ãƒãƒƒã‚°ç”¨: ãƒšãƒ¼ã‚¸å†…å®¹ã®ä¸€éƒ¨ã‚’è¡¨ç¤º
            print("\nğŸ” ãƒ‡ãƒãƒƒã‚°æƒ…å ±:")
            print(f"ãƒšãƒ¼ã‚¸ã‚µã‚¤ã‚º: {len(html_content)} æ–‡å­—")
            
            # googleusercontent.com ã®æ–‡å­—åˆ—ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            if 'googleusercontent.com' in html_content:
                print("âœ… googleusercontent.comæ–‡å­—åˆ—ã¯å­˜åœ¨ã—ã¾ã™")
                
                # ã‚ˆã‚Šç·©ã„æ¡ä»¶ã§æ¤œç´¢
                loose_pattern = r'googleusercontent\.com[^"\']*'
                loose_matches = re.findall(loose_pattern, html_content)
                print(f"ğŸ” ç·©ã„æ¡ä»¶ã§ã®æ¤œç´¢: {len(loose_matches)}ä»¶")
                
                if loose_matches:
                    print("ã‚µãƒ³ãƒ—ãƒ«:")
                    for match in loose_matches[:3]:
                        print(f"  {match}")
            else:
                print("âŒ googleusercontent.comæ–‡å­—åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        return all_urls
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def clean_google_photos_urls(urls):
    """
    Google Photos URLã‹ã‚‰ã‚µã‚¤ã‚ºãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é™¤å»ã—ã¦ã‚¯ãƒªãƒ¼ãƒ³ãªURLã«ã™ã‚‹
    """
    cleaned_urls = []
    
    for url in urls:
        # URLãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é™¤å»ã—ã¦ãƒ™ãƒ¼ã‚¹URLã®ã¿å–å¾—
        if '=' in url:
            base_url = url.split('=')[0]
        else:
            base_url = url
        
        cleaned_urls.append(base_url)
    
    return cleaned_urls

def generate_markdown_images(urls, base_alt_text="ç”»åƒ"):
    """
    URLã‹ã‚‰Markdownç”»åƒè¨˜æ³•ã‚’ç”Ÿæˆ
    """
    markdown_lines = []
    
    for i, url in enumerate(urls, 1):
        alt_text = f"{base_alt_text}{i}"
        markdown = f"![{alt_text}]({url})"
        markdown_lines.append(markdown)
    
    return markdown_lines

def main():
    print("ğŸ”— Google Photos URLæŠ½å‡ºãƒ†ã‚¹ãƒˆ")
    print("=" * 50)
    
    # ãƒ†ã‚¹ãƒˆç”¨URL
    test_url = "https://photos.app.goo.gl/qctTRRMsqWvjHaPKA"
    
    # URLæŠ½å‡º
    extracted_urls = extract_google_photos_urls(test_url)
    
    if extracted_urls:
        print(f"\nâœ… {len(extracted_urls)}å€‹ã®ç”»åƒURLã‚’æŠ½å‡ºã—ã¾ã—ãŸ")
        
        # URLã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        cleaned_urls = clean_google_photos_urls(extracted_urls)
        
        print(f"\nğŸ“ è¨˜äº‹ç”¨Markdownç”Ÿæˆ:")
        print("=" * 30)
        
        markdown_images = generate_markdown_images(cleaned_urls)
        for markdown in markdown_images:
            print(markdown)
        
        print(f"\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
        print("ä¸Šè¨˜ã®Markdownã‚’è¨˜äº‹ã«ã‚³ãƒ”ãƒ¼ï¼†ãƒšãƒ¼ã‚¹ãƒˆã—ã¦ãã ã•ã„")
        
    else:
        print(f"\nâŒ ç”»åƒURLã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ")
        print(f"ğŸ’¡ æ‰‹å‹•ã§ã®å¯¾å¿œã‚’ãŠå‹§ã‚ã—ã¾ã™:")
        print(f"1. {test_url} ã‚’ãƒ–ãƒ©ã‚¦ã‚¶ã§é–‹ã")
        print(f"2. å„ç”»åƒã§å³ã‚¯ãƒªãƒƒã‚¯â†’'ç”»åƒã®ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’ã‚³ãƒ”ãƒ¼'")
        print(f"3. URLã®æœ«å°¾ã‚’ '=w800-s-no-gm' ã«å¤‰æ›´")

if __name__ == "__main__":
    main()