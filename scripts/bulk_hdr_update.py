#!/usr/bin/env python3
"""
æ—¢å­˜è¨˜äº‹ã®ç”»åƒURLã‚’HDRç‰ˆã«ä¸€æ‹¬æ›´æ–°ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ä½¿ç”¨æ–¹æ³•:
python bulk_hdr_update.py --dry-run    # ç¢ºèªç”¨å®Ÿè¡Œ
python bulk_hdr_update.py              # å®Ÿéš›ã®æ›´æ–°
python bulk_hdr_update.py --file "è¨˜äº‹ãƒ•ã‚¡ã‚¤ãƒ«å.md"  # ç‰¹å®šè¨˜äº‹ã®ã¿

æ©Ÿèƒ½:
- content/posts/*.mdã‹ã‚‰=s1621ãƒ‘ã‚¿ãƒ¼ãƒ³ã®URLã‚’æ¤œå‡º
- å„URLã‹ã‚‰å®Ÿéš›ã®ç”»åƒã‚µã‚¤ã‚ºã‚’å–å¾—
- 800pxå¹…åŸºæº–ã§HDR URLã‚’ç”Ÿæˆ
- è¨˜äº‹ãƒ•ã‚¡ã‚¤ãƒ«å†…ã®URLã‚’ä¸€æ‹¬ç½®æ›
- å‡¦ç†çµ±è¨ˆã¨ãƒ¬ãƒãƒ¼ãƒˆã‚’è¡¨ç¤º
"""

import os
import re
import sys
import glob
import argparse
from pathlib import Path
from collections import defaultdict

def convert_url_to_s800_no_gm(original_url):
    """Google Photos URLã‚’s800-no-gmå½¢å¼ã«å¤‰æ›ï¼ˆå¤§å¹…ç°¡ç´ åŒ–ï¼‰"""
    # s1621å½¢å¼ã®å¤‰æ›
    if '=s1621?authuser=0' in original_url:
        return original_url.replace('=s1621?authuser=0', '=s800-no-gm?authuser=0')
    elif '=s1621' in original_url:
        return original_url.replace('=s1621', '=s800-no-gm')
    
    # æ—¢å­˜HDRå½¢å¼ã®å¤‰æ›
    # w{width}-h{height}-s-no-gm â†’ s800-no-gm
    import re
    hdr_pattern = r'=w\d+-h\d+-s-no-gm'
    if re.search(hdr_pattern, original_url):
        return re.sub(hdr_pattern, '=s800-no-gm', original_url)
    
    # å¤‰æ›ä¸è¦ã®å ´åˆ
    return original_url


def scan_markdown_files(target_file=None):
    """Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã—ã¦å¤‰æ›å¯¾è±¡ã®Google Photos URLã‚’æŠ½å‡º"""
    content_dir = Path("content/posts")
    
    if target_file:
        files = [content_dir / target_file]
    else:
        files = list(content_dir.glob("*.md"))
    
    # s1621å½¢å¼ã¨HDRå½¢å¼ï¼ˆw{width}-h{height}-s-no-gmï¼‰ã®ä¸¡æ–¹ã‚’æ¤œå‡º
    patterns = [
        re.compile(r'https://lh3\.googleusercontent\.com/[^)\s]*=s1621[^)\s]*'),
        re.compile(r'https://lh3\.googleusercontent\.com/[^)\s]*=w\d+-h\d+-s-no-gm[^)\s]*')
    ]
    
    file_urls = {}  # ãƒ•ã‚¡ã‚¤ãƒ«å -> URLãƒªã‚¹ãƒˆ
    all_urls = set()  # é‡è¤‡æ’é™¤ç”¨
    
    for file_path in files:
        if not file_path.exists():
            print(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
            continue
            
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ä¸¡æ–¹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§æ¤œç´¢
            urls = []
            for pattern in patterns:
                urls.extend(pattern.findall(content))
            
            if urls:
                file_urls[file_path.name] = urls
                all_urls.update(urls)
        except Exception as e:
            print(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {file_path.name}: {e}")
    
    return file_urls, list(all_urls)

def process_urls_to_s800(urls):
    """URLãƒªã‚¹ãƒˆã‚’s800-no-gmå½¢å¼ã«å¤‰æ›ï¼ˆã‚·ãƒ³ãƒ—ãƒ«å‡¦ç†ï¼‰"""
    url_mapping = {}  # å…ƒURL -> å¤‰æ›å¾ŒURL
    
    print(f"\nğŸ”„ {len(urls)}å€‹ã®URLã‚’s800-no-gmå½¢å¼ã«å¤‰æ›ä¸­...")
    
    for i, url in enumerate(urls, 1):
        converted_url = convert_url_to_s800_no_gm(url)
        url_mapping[url] = converted_url
        
        if converted_url != url:
            print(f"[{i:2d}/{len(urls)}] âœ… å¤‰æ›: {url.split('=')[-1][:20]}... â†’ s800-no-gm")
        else:
            print(f"[{i:2d}/{len(urls)}] â– å¤‰æ›ä¸è¦: ã™ã§ã«æœ€é©åŒ–æ¸ˆã¿")
    
    return url_mapping

def update_markdown_files(file_urls, url_mapping, dry_run=False):
    """Markdownãƒ•ã‚¡ã‚¤ãƒ«å†…ã®URLã‚’æ›´æ–°"""
    content_dir = Path("content/posts")
    updated_files = 0
    total_replacements = 0
    
    print(f"\n{'ğŸ” [DRY RUN]' if dry_run else 'âœï¸'} è¨˜äº‹ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°ä¸­...")
    
    for filename, urls in file_urls.items():
        file_path = content_dir / filename
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            original_content = content
            file_replacements = 0
            
            # å„URLã‚’ç½®æ›
            for old_url in urls:
                new_url = url_mapping.get(old_url, old_url)
                if new_url != old_url:
                    content = content.replace(old_url, new_url)
                    file_replacements += 1
            
            if file_replacements > 0:
                if not dry_run:
                    with open(file_path, 'w', encoding='utf-8') as f:
                        f.write(content)
                
                updated_files += 1
                total_replacements += file_replacements
                print(f"   âœ… {filename}: {file_replacements}å€‹ã®URLæ›´æ–°")
            else:
                print(f"   ğŸ“· {filename}: æ›´æ–°ä¸è¦")
                
        except Exception as e:
            print(f"   âŒ {filename}: æ›´æ–°ã‚¨ãƒ©ãƒ¼ - {e}")
    
    return updated_files, total_replacements

def main():
    parser = argparse.ArgumentParser(description='æ—¢å­˜è¨˜äº‹ã®ç”»åƒURLã‚’s800-no-gmå½¢å¼ã«ä¸€æ‹¬å¤‰æ›')
    parser.add_argument('--dry-run', action='store_true', help='å®Ÿéš›ã®æ›´æ–°ã‚’è¡Œã‚ãšã€ç¢ºèªã®ã¿å®Ÿè¡Œ')
    parser.add_argument('--file', type=str, help='ç‰¹å®šã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å‡¦ç†')
    
    args = parser.parse_args()
    
    print("=" * 70)
    print("ğŸš€ æ—¢å­˜è¨˜äº‹ç”»åƒURL s800-no-gmä¸€æ‹¬å¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    print("=" * 70)
    
    if args.dry_run:
        print("ğŸ” DRY RUNãƒ¢ãƒ¼ãƒ‰: å®Ÿéš›ã®æ›´æ–°ã¯è¡Œã„ã¾ã›ã‚“")
    
    # Step 1: Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒ£ãƒ³
    print("\nğŸ“‚ è¨˜äº‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ã‚­ãƒ£ãƒ³ä¸­...")
    file_urls, all_urls = scan_markdown_files(args.file)
    
    if not all_urls:
        print("âœ¨ å¤‰æ›å¯¾è±¡ã®Google Photos URLãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return
    
    print(f"ğŸ“Š æ¤œå‡ºçµæœ:")
    print(f"   ğŸ“„ å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(file_urls)}å€‹")
    print(f"   ğŸ”— ãƒ¦ãƒ‹ãƒ¼ã‚¯URLæ•°: {len(all_urls)}å€‹")
    
    # Step 2: URLã‚’s800-no-gmå½¢å¼ã«å¤‰æ›
    url_mapping = process_urls_to_s800(all_urls)
    
    # å¤‰æ›çµ±è¨ˆ
    converted_count = sum(1 for old, new in url_mapping.items() if old != new)
    print(f"\nğŸ“ˆ å¤‰æ›çµ±è¨ˆ:")
    print(f"   âœ… s800-no-gmå¤‰æ›æˆåŠŸ: {converted_count}å€‹")
    print(f"   ğŸ“· å¤‰æ›ä¸è¦: {len(all_urls) - converted_count}å€‹")
    
    # Step 3: ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°
    if converted_count > 0:
        updated_files, total_replacements = update_markdown_files(file_urls, url_mapping, args.dry_run)
        
        print(f"\nğŸ“‹ æœ€çµ‚çµæœ:")
        print(f"   ğŸ“„ æ›´æ–°ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {updated_files}å€‹")
        print(f"   ğŸ”„ ç·ç½®æ›å›æ•°: {total_replacements}å›")
        
        if args.dry_run:
            print(f"\nğŸ’¡ å®Ÿéš›ã«æ›´æ–°ã™ã‚‹ã«ã¯ä»¥ä¸‹ã‚’å®Ÿè¡Œ:")
            if args.file:
                print(f"   python scripts/bulk_hdr_update.py --file \"{args.file}\"")
            else:
                print(f"   python scripts/bulk_hdr_update.py")
        else:
            print(f"\nâœ… s800-no-gmä¸€æ‹¬å¤‰æ›å®Œäº†ï¼")
    else:
        print(f"\nğŸ“· å¤‰æ›å¯¾è±¡ã®URLãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nâš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)