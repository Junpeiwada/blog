#!/usr/bin/env python3
"""
ã‚¹ãƒãƒ¼ãƒˆGoogle Photos URLç”Ÿæˆ - ãƒ™ãƒ¼ã‚¹URLã‹ã‚‰ãƒ•ãƒ«ã‚µã‚¤ã‚ºURLè‡ªå‹•ç”Ÿæˆ
"""

import re
import requests

def analyze_user_provided_urls():
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼æä¾›ã®ãƒ•ãƒ«ã‚µã‚¤ã‚ºURLã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ
    """
    sample_urls = [
        "https://lh3.googleusercontent.com/pw/AP1GczPh2vPU8BavZbRjLKN7P8G1g2z9TzVX9mvR2Gwu6hWqfa58t7Epuga5SPcWvEVHemWssozaUmLxktvxLF9NT2ehXPKwzEpEauBxZgsuCcpIyOrAQ6yQeGpCCS_42EAtp_DisEXd3Q7--gLAxhnzK0tMjA=w4238-h2382-s-no-gm?authuser=0",
        "https://lh3.googleusercontent.com/pw/AP1GczMITnmFejNzG5ft5rKYNfmaJWuT7GD-VbZm9K5lGmRHOVO4L3VaFifm5f1ZBJGuIpyYoayJ_Tc1RJ4-0ncQ40Sv3CtGvPRL9BQR_AqYNxh6BuX5nO4j6Kb-NJgTdmJDzMuetpkzWuek1uCpYcrFSUoolw=w4238-h2384-s-no-gm?authuser=0"
    ]
    
    print("ğŸ” ãƒ¦ãƒ¼ã‚¶ãƒ¼æä¾›URLãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ:")
    
    for i, url in enumerate(sample_urls, 1):
        print(f"   {i}. ãƒ™ãƒ¼ã‚¹URL: {url.split('=')[0]}")
        print(f"      ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {url.split('=', 1)[1]}")
    
    return {
        "base_pattern": r"(https://lh[0-9]+\.googleusercontent\.com/pw/[A-Za-z0-9_-]+)",
        "fullsize_suffix": "=w4238-h2382-s-no-gm?authuser=0",
        "alternative_suffix": "=w4238-h2384-s-no-gm?authuser=0"
    }

def extract_base_urls_with_simple_requests(shared_link):
    """
    ã‚·ãƒ³ãƒ—ãƒ«ãªrequestsã§ãƒ™ãƒ¼ã‚¹URLã‚’æŠ½å‡º
    """
    print(f"ğŸŒ ã‚·ãƒ³ãƒ—ãƒ«HTTPå–å¾—ã§ãƒ™ãƒ¼ã‚¹URLæŠ½å‡º...")
    
    try:
        response = requests.get(shared_link, allow_redirects=True)
        html_content = response.text
        
        print(f"ğŸ“„ HTMLã‚µã‚¤ã‚º: {len(html_content)} æ–‡å­—")
        
        # ãƒ™ãƒ¼ã‚¹URLãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢
        base_pattern = r'https://lh[0-9]+\.googleusercontent\.com/pw/[A-Za-z0-9_-]+'
        base_urls = re.findall(base_pattern, html_content)
        
        # é‡è¤‡é™¤å»ã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        unique_base_urls = list(set(base_urls))
        
        # é•·ã•ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆçŸ­ã™ãã‚‹URLã¯é™¤å¤–ï¼‰
        filtered_base_urls = [url for url in unique_base_urls if len(url) > 80]
        
        print(f"âœ… ãƒ™ãƒ¼ã‚¹URLå–å¾—: {len(filtered_base_urls)}å€‹")
        
        return filtered_base_urls
        
    except Exception as e:
        print(f"âŒ ãƒ™ãƒ¼ã‚¹URLå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def generate_fullsize_urls_from_base(base_urls, patterns):
    """
    ãƒ™ãƒ¼ã‚¹URLã‹ã‚‰ãƒ•ãƒ«ã‚µã‚¤ã‚ºURLã‚’ç”Ÿæˆ
    """
    print(f"ğŸ› ï¸  ãƒ•ãƒ«ã‚µã‚¤ã‚ºURLç”Ÿæˆä¸­...")
    
    fullsize_urls = []
    
    for base_url in base_urls:
        # è¤‡æ•°ã®ã‚µã‚¤ã‚ºãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦è¡Œ
        size_patterns = [
            "=w4238-h2382-s-no-gm?authuser=0",  # æ¨™æº–ãƒ•ãƒ«ã‚µã‚¤ã‚º
            "=w4238-h2384-s-no-gm?authuser=0",  # åˆ¥ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”
            "=w3840-h2160-s-no-gm?authuser=0",  # 4K UHD
            "=w4096-h3072-s-no-gm?authuser=0",  # 4:3ã‚¢ã‚¹ãƒšã‚¯ãƒˆ
        ]
        
        for pattern in size_patterns:
            fullsize_url = base_url + pattern
            fullsize_urls.append(fullsize_url)
    
    return fullsize_urls

def verify_url_accessibility(urls):
    """
    URLã®ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½æ€§ã‚’æ¤œè¨¼
    """
    print(f"ğŸ”— URLæœ‰åŠ¹æ€§æ¤œè¨¼ä¸­...")
    
    valid_urls = []
    
    for i, url in enumerate(urls, 1):
        try:
            # HEADãƒªã‚¯ã‚¨ã‚¹ãƒˆã§å­˜åœ¨ç¢ºèª
            response = requests.head(url, timeout=10)
            
            if response.status_code == 200:
                print(f"   âœ… URL{i}: æœ‰åŠ¹")
                valid_urls.append(url)
            else:
                print(f"   âŒ URL{i}: {response.status_code}")
                
        except Exception as e:
            print(f"   âŒ URL{i}: ã‚¢ã‚¯ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼")
    
    return valid_urls

def main():
    print("ğŸ§  ã‚¹ãƒãƒ¼ãƒˆGoogle Photos ãƒ•ãƒ«ã‚µã‚¤ã‚ºURLç”Ÿæˆ")
    print("=" * 60)
    print("æ‰‹æ³•: ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ + ãƒ™ãƒ¼ã‚¹URLæŠ½å‡º + ãƒ•ãƒ«ã‚µã‚¤ã‚ºç”Ÿæˆ")
    
    test_url = "https://photos.app.goo.gl/qctTRRMsqWvjHaPKA"
    
    # 1. ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
    patterns = analyze_user_provided_urls()
    
    # 2. ãƒ™ãƒ¼ã‚¹URLæŠ½å‡º
    base_urls = extract_base_urls_with_simple_requests(test_url)
    
    if base_urls:
        print(f"\nğŸ“‹ ãƒ™ãƒ¼ã‚¹URLä¸€è¦§:")
        for i, url in enumerate(base_urls, 1):
            print(f"   {i}. {url}")
        
        # 3. ãƒ•ãƒ«ã‚µã‚¤ã‚ºURLç”Ÿæˆ
        generated_urls = generate_fullsize_urls_from_base(base_urls, patterns)
        
        # 4. URLæœ‰åŠ¹æ€§æ¤œè¨¼
        valid_urls = verify_url_accessibility(generated_urls[:20])  # æœ€å¤§20å€‹ã¾ã§æ¤œè¨¼
        
        if valid_urls:
            print(f"\nâœ… {len(valid_urls)}å€‹ã®æœ‰åŠ¹ãªãƒ•ãƒ«ã‚µã‚¤ã‚ºURLã‚’ç”Ÿæˆï¼")
            
            print(f"\nğŸ“‹ æœ‰åŠ¹ãªãƒ•ãƒ«ã‚µã‚¤ã‚ºURL:")
            print("=" * 50)
            
            for i, url in enumerate(valid_urls, 1):
                print(f"{i:2d}. {url}")
            
            print(f"\nğŸ“ è¨˜äº‹ç”¨Markdown:")
            print("=" * 30)
            
            for i, url in enumerate(valid_urls, 1):
                print(f"![ç”»åƒ{i}]({url})")
        else:
            print(f"\nâŒ æœ‰åŠ¹ãªãƒ•ãƒ«ã‚µã‚¤ã‚ºURLã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸ")
    else:
        print(f"\nâŒ ãƒ™ãƒ¼ã‚¹URLã®æŠ½å‡ºã«å¤±æ•—")

if __name__ == "__main__":
    main()